{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "coated-expression",
   "metadata": {},
   "source": [
    "# Practice Querying the Snowexsql Database\n",
    "\n",
    "(12 minutes)\n",
    "\n",
    "Learning Objectives:  \n",
    "- First taste of the database!\n",
    "- Code snippets to extract and prep data.\n",
    "- Generate ideas for project pitches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "republican-posting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import datetime\n",
    "\n",
    "#database imports\n",
    "from snowexsql.db import get_db\n",
    "from snowexsql.data import PointData, LayerData, ImageData, SiteData\n",
    "from snowexsql.conversions import query_to_geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-transportation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the database\n",
    "db_name = 'snow:hackweek@52.32.183.144/snowex'\n",
    "engine, session = get_db(db_name)\n",
    "\n",
    "print('snowexsql database successfully loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-valley",
   "metadata": {},
   "source": [
    "## Snow Pit data are contained in the following data tables:  \n",
    "\n",
    "_PointData_  = pit ruler depths, SWE.  \n",
    "_LayerData_  = density, temperature, stratigraphy, etc.  \n",
    "_SiteData_   = siteID, airTemp, vegetation, visit time, weather, etc.  \n",
    "\n",
    "### Example 1: Let's find all the pits that overlap with an airborne sensor of interest!\n",
    "\n",
    "First, it would be helpful to know, which of the airborne sensors are part of the database, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-gazette",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the session using .surveyors() to generate a list\n",
    "qry = session.query(ImageData.surveyors)\n",
    "\n",
    "# Locate all that are distinct\n",
    "airborne_sensors_list = session.query(ImageData.surveyors).distinct().all()\n",
    "\n",
    "print('list of airborne sensors by \"surveyor\" name: \\n', airborne_sensors_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-conversation",
   "metadata": {},
   "source": [
    "#### 1a). Unsure of the flight date, but know which sensor you'd like to overlap with, here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# airborne sensor from list above\n",
    "sensor = 'UAVSAR team, JPL'\n",
    "\n",
    "# Form on the Images table that returns Raster collection dates\n",
    "qry = session.query(ImageData.date)\n",
    "\n",
    "# Filter for UAVSAR data\n",
    "qry = qry.filter(ImageData.surveyors == sensor)\n",
    "\n",
    "# Grab the unique dates\n",
    "qry = qry.distinct()\n",
    "\n",
    "# Execute the query \n",
    "dates = qry.all() \n",
    "\n",
    "# Clean up the dates \n",
    "dates = [d[0] for d in dates] \n",
    "dlist = [str(d) for d in dates]\n",
    "dlist = \", \".join(dlist)\n",
    "print('%s flight dates are: %s' %(sensor, dlist))\n",
    "\n",
    "# Find all the snow pits done on these days\n",
    "qry = session.query(SiteData.geom, SiteData.site_id, SiteData.date)\n",
    "qry = qry.filter(SiteData.date.in_(dates))\n",
    "\n",
    "# return a geopandas df\n",
    "df = query_to_geopandas(qry, engine)\n",
    "df.head()\n",
    "\n",
    "# Close your session to avoid hanging transactions\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-testing",
   "metadata": {},
   "source": [
    "#### 1b). Want to select an exact flight date match? Here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-quantity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a day from the list of dates\n",
    "dt = dates[0] \n",
    "\n",
    "# Find all the snow pits done on these days \n",
    "qry = session.query(SiteData.geom, SiteData.site_id, SiteData.date)\n",
    "qry = qry.filter(SiteData.date == dt)\n",
    "\n",
    "# return a geopandas df\n",
    "df_exact = query_to_geopandas(qry, engine)\n",
    "\n",
    "print('%s pits overlap with %s on %s' %(len(df_exact), sensor, dt))\n",
    "df_exact.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-namibia",
   "metadata": {},
   "source": [
    "#### 1c). Want to select a range of dates near the flight date? Here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-mailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form a date range to query on either side of our chosen day \n",
    "date_range = [dt + i * datetime.timedelta(days=1) for i in [-1, 0, 1]]\n",
    "\n",
    "# Find all the snow pits done on these days and return a geopandas df\n",
    "qry = session.query(SiteData.geom, SiteData.site_id, SiteData.date)\n",
    "qry = qry.filter(SiteData.date.in_(date_range))\n",
    "\n",
    "# return a geopandas df\n",
    "df_range = query_to_geopandas(qry, engine)\n",
    "\n",
    "# clean up dates (for print statement only)\n",
    "dlist = [str(d) for d in date_range]\n",
    "dlist = \", \".join(dlist)\n",
    "\n",
    "print('%s pits overlap with %s on %s' %(len(df_range), sensor, dlist))\n",
    "df_range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-daughter",
   "metadata": {},
   "source": [
    "#### 1d). Have a known date in mind that you wish to select data for, here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-request",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the data that was collected on 2-12-2020\n",
    "dt = datetime.date(2020, 2, 12)\n",
    "\n",
    "# Grab all Point data instruments from our date\n",
    "point_instruments = session.query(PointData.instrument).filter(PointData.date == dt).distinct().all()\n",
    "print('Point Data on %s are: \\n%s', (str(dt), point_instruments))\n",
    "\n",
    "# Grab all Layer data instruments from our date\n",
    "layer_instruments = session.query(LayerData.instrument, LayerData.type).filter(LayerData.date == dt).distinct().all()\n",
    "print('\\nLayer Data are: \\n', layer_instruments)\n",
    "\n",
    "# Grab all Image data instruments from our date\n",
    "image_instruments = session.query(ImageData.instrument).filter(ImageData.date == dt).distinct().all()\n",
    "print('\\nImage Data are: \\n', image_instruments)\n",
    "\n",
    "qry = session.query(PointData.geom, PointData.instrument, PointData.site_id, PointData.date)\n",
    "qry = qry.filter(PointData.date == dt) \n",
    "df = query_to_geopandas(qry, engine)\n",
    "df\n",
    "\n",
    "qry = session.query(LayerData.geom, LayerData.instrument, LayerData.site_id, LayerData.date)\n",
    "qry = qry.filter(LayerData.date == dt) \n",
    "df = query_to_geopandas(qry, engine)\n",
    "df\n",
    "\n",
    "qry = session.query(ImageData.instrument, ImageData.site_id, ImageData.date)\n",
    "qry = qry.filter(ImageData.date == dt) \n",
    "qry = qry.distinct()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transparent-questionnaire",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "str(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-pride",
   "metadata": {},
   "source": [
    "### Nice work, almost done here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-september",
   "metadata": {},
   "source": [
    "## Classify pit data based on the depth and vegetation matrix\n",
    "### Example 2: \n",
    "\n",
    "#### 2a).Distinguish pits by vegetation coverage: \n",
    "- treeless (0% tree cover)\n",
    "- sparse (1-30% tree cover)\n",
    "- dense (31-100% tree cover)\n",
    "\n",
    "*vegetation classes assigned based on optical imagery: tree density map, Nov. 2010 WorldView-2 Imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-compatibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_veg_class(site_id):\n",
    "    \n",
    "        '''\n",
    "    This function parses snow pit data into three vegetation classes:\n",
    "        - 1). Treeless, 2). Sparce, and 3). Dense\n",
    "        \n",
    "    It uses a python dictionary where:\n",
    "        (k) keys: are the vegetation classes\n",
    "        (v) values: are the first digit in the pitID assignment\n",
    "\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Classifying by vegetation coverage \n",
    "    veg_class = {'treeless':[1, 2, 3], 'sparse':[4, 5, 6], 'dense':[7, 8, 9]}\n",
    "   \n",
    "    vclass = None \n",
    "    \n",
    "    class_id = site_id[0]\n",
    "    \n",
    "    if class_id.isnumeric():\n",
    "        class_id = int(class_id)\n",
    "\n",
    "        for k,v in veg_class.items():\n",
    "\n",
    "            if class_id in v: #if the first digit in the site_id is 'v' assign it to the corresponding 'k'\n",
    "                vclass = k \n",
    "                \n",
    "    return vclass "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-smoke",
   "metadata": {},
   "source": [
    "#### 2b). Distinguish pits by snow depth classes: \n",
    "- shallow (<90cm)\n",
    "- medium (90-122cm)\n",
    "- deep (>122cm)\n",
    "\n",
    "*depth classes assigned based on 2017 snow depth lidar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-approach",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_depth_class(site_id):\n",
    "    \n",
    "            '''\n",
    "    This function parses snow pit data into three depth classes:\n",
    "        - 1). Shallow, 2). Medium, and 3). Deep\n",
    "        \n",
    "    It uses a python dictionary where:\n",
    "        (k) keys: are the depth classes\n",
    "        (v) values: are the first digit in the pitID assignment\n",
    "      \n",
    "  \n",
    "    '''\n",
    "        \n",
    "    # Classifying by expected depth \n",
    "    depth_class = {'shallow':[1, 4, 7], 'medium':[2, 5, 8], 'deep':[3, 6, 9]} \n",
    "   \n",
    "    dclass = None \n",
    "    \n",
    "    class_id = site_id[0]\n",
    "    \n",
    "    if class_id.isnumeric(): #for the outlier TS site\n",
    "        class_id = int(class_id) #cast as integer\n",
    "\n",
    "        for k,v in depth_class.items(): #for the key, value pairs in the dict listed above:\n",
    "\n",
    "            if class_id in v: #if the first digit in the site_id is 'v' assign it to the corresponding 'k'\n",
    "                dclass = k \n",
    "\n",
    "    return dclass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-colony",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the database\n",
    "db_name = 'snow:hackweek@52.32.183.144/snowex'\n",
    "engine, session = get_db(db_name)\n",
    "\n",
    "result = session.query(LayerData.type).distinct().all()\n",
    "\n",
    "qry = session.query(LayerData).filter(LayerData.type=='density')\n",
    "\n",
    "# Form our dataframe from the query \n",
    "df = query_to_geopandas(qry, engine)\n",
    "df['value'] = df['value'].astype(float) #cast the value as a float (they are strings)\n",
    " \n",
    "# parse snow pit data by the veg/depth matrix\n",
    "df['veg_class'] = [parse_veg_class(i) for i in df['site_id']] #run the parse_veg function for every site_id\n",
    "df['depth_class'] = [parse_depth_class(i) for i in df['site_id']] #run the parse_depth funciton for every site_id\n",
    "\n",
    "# # Show off our df \n",
    "# df.plot()\n",
    "\n",
    "df.columns\n",
    "col_list = ['site_name', 'date', 'id', 'instrument', 'type', 'units', 'surveyors', 'latitude',\n",
    "       'longitude', 'geom','depth', 'site_id', 'value', 'veg_class', 'depth_class']\n",
    "df = df[col_list]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-pontiac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all of the unique site ids \n",
    "len(df['site_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-payment",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make df with only 153 rows using site_id.unique\n",
    "gb = df.groupby(['site_id', 'veg_class'])\n",
    "#gb = df.groupby('site_id')\n",
    "gb['site_name'].count().groupby(\"veg_class\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-magic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = df[['veg_class', 'depth_class']].groupby(df['site_id']).groups\n",
    "# gb = df.groupby(['veg_class', 'site_id']).count()\n",
    "# gb = df.groupby(['site_id']).count()\n",
    "# df['veg_class'].groupby('veg_class').count()\n",
    "\n",
    "# print(gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-sport",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['site_id'].unique().groupby('veg_class').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-anatomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['veg_class', 'site_id']].groupby('veg_class').count() #table I like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-murray",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Great for debugging especially when trying different queries\n",
    "# session.rollback()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
