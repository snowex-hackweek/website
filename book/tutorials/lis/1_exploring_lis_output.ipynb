{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "surprising-matrix",
   "metadata": {},
   "source": [
    "# Visualizing and Comparing LIS Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-month",
   "metadata": {},
   "source": [
    "```{figure} ./images/nasa-lis-combined-logos.png\n",
    "---\n",
    "width: 300px\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-april",
   "metadata": {},
   "source": [
    "## LIS Output Primer\n",
    "\n",
    "LIS writes model state variables to disk at a frequency selected by the user (e.g., 6-hourly, daily, monthly). The LIS output we will be exploring was originally generated as *daily* NetCDF files, meaning one NetCDF was written per simulated day. We have converted these NetCDF files into a [Zarr](https://zarr.readthedocs.io/en/stable/) store for improved performance in the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-positive",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-grammar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interface to Amazon S3 filesystem\n",
    "import s3fs\n",
    "\n",
    "# interact with n-d arrays\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "# interact with tabular data (incl. spatial)\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# interactive plots\n",
    "import holoviews as hv\n",
    "import geoviews as gv\n",
    "import hvplot.pandas\n",
    "import hvplot.xarray\n",
    "\n",
    "# used to find nearest grid cell to a given location\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# set bokeh as the holoviews plotting backend\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-mouse",
   "metadata": {},
   "source": [
    "## Load the LIS Output\n",
    "\n",
    "The `xarray` library makes working with labelled n-dimensional arrays easy and efficient. If you're familiar with the `pandas` library it should feel pretty familiar.\n",
    "\n",
    "Here we load the LIS output into an `xarray.Dataset` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-delaware",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create S3 filesystem object\n",
    "s3 = s3fs.S3FileSystem(anon=False)\n",
    "\n",
    "# define the name of our S3 bucket\n",
    "bucket_name = 'eis-dh-hydro'\n",
    "\n",
    "# define path to store on S3\n",
    "lis_output_s3_path = f's3://{bucket_name}/SNOWEX-HACKWEEK/DA_SNODAS/SURFACEMODEL/LIS_HIST.d01.zarr/'\n",
    "\n",
    "# create key-value mapper for S3 object (required to read data stored on S3)\n",
    "lis_output_mapper = s3.get_mapper(lis_output_s3_path)\n",
    "\n",
    "# open the dataset\n",
    "lis_output_ds = xr.open_zarr(lis_output_mapper, consolidated=True)\n",
    "\n",
    "# drop some unneeded variables\n",
    "lis_output_ds = lis_output_ds.drop_vars(['_history', '_eis_source_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-closure",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "\n",
    "Display an interactive widget for inspecting the dataset by running a cell containing the variable name. Expand the dropdown menus and click on the document and database icons to inspect the variables and attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-florida",
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_output_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-church",
   "metadata": {},
   "source": [
    "### Accessing Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "likely-pricing",
   "metadata": {},
   "source": [
    "Dataset attributes (metadata) are accessible via the `attrs` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-pocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_output_ds.attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-creation",
   "metadata": {},
   "source": [
    "### Accessing Variables\n",
    "\n",
    "Variables can be accessed using either **dot notation** or **square bracket notation**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-sandwich",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dot notation\n",
    "lis_output_ds.SnowDepth_tavg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-council",
   "metadata": {},
   "outputs": [],
   "source": [
    "# square bracket notation\n",
    "lis_output_ds['SnowDepth_tavg']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-consciousness",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Which syntax should I use?\n",
    "While both syntaxes perform the same function, the square-bracket syntax is useful when interacting with a dataset programmatically. For example, we can define a variable `varname` that stores the name of the variable in the dataset we want to access and then use that with the square-brackets notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-colony",
   "metadata": {},
   "outputs": [],
   "source": [
    "varname = 'SnowDepth_tavg'\n",
    "\n",
    "lis_output_ds[varname]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-inclusion",
   "metadata": {},
   "source": [
    "The dot notation syntax will not work this way because `xarray` tries to find a variable in the dataset named `varname` instead of the value of the `varname` variable. When `xarray` can't find this variable, it throws an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-antenna",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run the code below to see the error\n",
    "\n",
    "# varname = 'SnowDepth_tavg'\n",
    "\n",
    "# lis_output_ds.varname"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-killer",
   "metadata": {},
   "source": [
    "### Dimensions and Coordinate Variables\n",
    "\n",
    "The dimensions and coordinate variable fields put the \"*labelled*\" in \"labelled n-dimensional arrays\":\n",
    "\n",
    "* **Dimensions:** labels for each dimension in the dataset (e.g., `time`)\n",
    "* **Coordinates:** labels for indexing along dimensions (e.g., `'2019-01-01'`)\n",
    "\n",
    "We can use these labels to select, slice, and aggregate the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-mechanism",
   "metadata": {},
   "source": [
    "#### Selecting/Subsetting\n",
    "\n",
    "`xarray` provides two methods for selecting or subsetting along coordinate variables:\n",
    "\n",
    "* index selection: `ds.isel(time=0)`\n",
    "* value selection `ds.sel(time='2019-01-01')`\n",
    "\n",
    "For example, we can select the first timestep from our dataset using index selection by passing the dimension name as a keyword argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-minneapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember: python indexes start at 0\n",
    "lis_output_ds.isel(time=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-magnitude",
   "metadata": {},
   "source": [
    "Or we can use value selection to select based on the coordinate(s) (think \"labels\") of a given dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-nomination",
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_output_ds.sel(time='2018-01-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-guest",
   "metadata": {},
   "source": [
    "The `.sel()` approach also allows the use of shortcuts in some cases. For example, here we select all timesteps in the month of January 2018:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-michigan",
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_output_ds.sel(time='2018-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-float",
   "metadata": {},
   "source": [
    "Select a custom range of dates using Python's built-in `slice()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-purple",
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_output_ds.sel(time=slice('2018-01-01', '2018-01-15'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-banana",
   "metadata": {},
   "source": [
    "#### Latitude and Longitude\n",
    "\n",
    "You may have noticed that latitude (`lat`) and longitude (`lon`) are listed as data variables, not coordinate variables. This dataset would be easier to work with if `lat` and `lon` were coordinate variables and dimensions. Here we define a helper function that reads the spatial information from the dataset attributes, generates arrays containing the `lat` and `lon` values, and appends them to the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-vienna",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_latlon_coords(dataset: xr.Dataset)->xr.Dataset:\n",
    "    \"\"\"Adds lat/lon as dimensions and coordinates to an xarray.Dataset object.\"\"\"\n",
    "    \n",
    "    # get attributes from dataset\n",
    "    attrs = dataset.attrs\n",
    "    \n",
    "    # get x, y resolutions\n",
    "    dx = round(float(attrs['DX']), 3)\n",
    "    dy = round(float(attrs['DY']), 3)\n",
    "    \n",
    "    # get grid cells in x, y dimensions\n",
    "    ew_len = len(dataset['east_west'])\n",
    "    ns_len = len(dataset['north_south'])\n",
    "    \n",
    "    # get lower-left lat and lon\n",
    "    ll_lat = round(float(attrs['SOUTH_WEST_CORNER_LAT']), 3)\n",
    "    ll_lon = round(float(attrs['SOUTH_WEST_CORNER_LON']), 3)\n",
    "    \n",
    "    # calculate upper-right lat and lon\n",
    "    ur_lat =  ll_lat + (dy * ns_len)\n",
    "    ur_lon = ll_lon + (dx * ew_len)\n",
    "    \n",
    "    # define the new coordinates\n",
    "    coords = {\n",
    "        # create an arrays containing the lat/lon at each gridcell\n",
    "        'lat': np.linspace(ll_lat, ur_lat, ns_len, dtype=np.float32, endpoint=False),\n",
    "        'lon': np.linspace(ll_lon, ur_lon, ew_len, dtype=np.float32, endpoint=False)\n",
    "    }\n",
    "    \n",
    "    lon_attrs = dataset.lon.attrs\n",
    "    lat_attrs = dataset.lat.attrs\n",
    "    \n",
    "    # rename the original lat and lon variables\n",
    "    dataset = dataset.rename({'lon':'orig_lon', 'lat':'orig_lat'})\n",
    "    # rename the grid dimensions to lat and lon\n",
    "    dataset = dataset.rename({'north_south': 'lat', 'east_west': 'lon'})\n",
    "    # assign the coords above as coordinates\n",
    "    dataset = dataset.assign_coords(coords)\n",
    "    dataset.lon.attrs = lon_attrs\n",
    "    dataset.lat.attrs = lat_attrs\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experimental-fiber",
   "metadata": {},
   "source": [
    "Now that the function is defined, let's use it to append `lat` and `lon` coordinates to the LIS output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-theory",
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_output_ds = add_latlon_coords(lis_output_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attended-paris",
   "metadata": {},
   "source": [
    "Inspect the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-luther",
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_output_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-intent",
   "metadata": {},
   "source": [
    "Now `lat` and `lon` are listed as coordinate variables and have replaced the `north_south` and `east_west` dimensions. This will make it easier to spatially subset the dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-string",
   "metadata": {},
   "source": [
    "#### Basic Spatial Subsetting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-wales",
   "metadata": {},
   "source": [
    "We can use the `slice()` function we used above on the `lat` and `lon` dimensions to select data between a range of latitudes and longitudes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-scene",
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_output_ds.sel(lat=slice(37, 41), lon=slice(-110, -101))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "presidential-importance",
   "metadata": {},
   "source": [
    "Notice how the sizes of the `lat` and `lon` dimensions have decreased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-stone",
   "metadata": {},
   "source": [
    "#### Subset Across Multiple Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-omaha",
   "metadata": {},
   "source": [
    "Select snow depth for Jan 2017 within a range of lat/lon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-cleaners",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a range of dates to select\n",
    "wy_2018_slice = slice('2017-10-01', '2018-09-30')\n",
    "lat_slice = slice(37, 41)\n",
    "lon_slice = slice(-110, -101)\n",
    "\n",
    "# select the snow depth and subset to wy_2018_slice\n",
    "snd_CO_wy2018_ds = lis_output_ds['SnowDepth_tavg'].sel(time=wy_2018_slice, lat=lat_slice, lon=lon_slice)\n",
    "\n",
    "# inspect resulting dataset\n",
    "snd_CO_wy2018_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-emphasis",
   "metadata": {},
   "source": [
    "### Plotting\n",
    "\n",
    "We've imported two plotting libraries:\n",
    "\n",
    "* `matplotlib`: static plots\n",
    "* `hvplot`: interactive plots\n",
    "\n",
    "We can make a quick `matplotlib`-based plot for the subsetted data using the `.plot()` function supplied by `xarray.Dataset` objects. For this example, we'll select one day and plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-allocation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple matplotlilb plot\n",
    "snd_CO_wy2018_ds.sel(time='2018-01-01').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-handle",
   "metadata": {},
   "source": [
    "Similarly we can make an interactive plot using the `hvplot` accessor and specifying a `quadmesh` plot type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-handle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hvplot based map\n",
    "snd_CO_wy2018_ds.sel(time='2018-01-01').hvplot.quadmesh(geo=True, rasterize=True, project=True,\n",
    "                                                        xlabel='lon', ylabel='lat', cmap='viridis',\n",
    "                                                        tiles='EsriImagery')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funded-bible",
   "metadata": {},
   "source": [
    "Pan, zoom, and scroll around the map. Hover over the LIS data to see the data values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-manitoba",
   "metadata": {},
   "source": [
    "If we try to plot more than one time-step `hvplot` will also provide a time-slider we can use to scrub back and forth in time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-works",
   "metadata": {},
   "outputs": [],
   "source": [
    "snd_CO_wy2018_ds.sel(time='2018-01').hvplot.quadmesh(geo=True, rasterize=True, project=True,\n",
    "                             xlabel='lon', ylabel='lat', cmap='viridis',\n",
    "                             tiles='EsriImagery')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-chile",
   "metadata": {},
   "source": [
    "From here on out we will stick with `hvplot` for plotting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaging-information",
   "metadata": {},
   "source": [
    "#### Timeseries Plots\n",
    "\n",
    "We can generate a timeseries for a given grid cell by selecting and calling the plot function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define point to take timeseries (note: must be present in coordinates of dataset)\n",
    "ts_lon, ts_lat = (-106.65, 40.25)\n",
    "\n",
    "# plot timeseries (hvplot knows how to plot based on dataset's dimensionality!)\n",
    "snd_CO_wy2018_ds.sel(lat=ts_lat, lon=ts_lon).hvplot(title=f'Snow Depth Timeseries @ Lon: {ts_lon}, Lat: {ts_lat}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-recycling",
   "metadata": {},
   "source": [
    "In the next section we'll learn how to create a timeseries over a broader area."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-tracker",
   "metadata": {},
   "source": [
    "## Aggregation\n",
    "\n",
    "We can perform aggregation operations on the dataset such as `min()`, `max()`, `mean()`, and `sum()` by specifying the dimensions along which to perform the calculation.\n",
    "\n",
    "For example we can calculate the mean and maximum snow depth at each grid cell over water year 2018 as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-capability",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean at each grid cell over the time dimension\n",
    "mean_snd_CO_wy2018_ds = snd_CO_wy2018_ds.mean(dim='time')\n",
    "max_snd_CO_wy2018_ds = snd_CO_wy2018_ds.max(dim='time')\n",
    "\n",
    "# plot the mean and max snow depth\n",
    "mean_snd_CO_wy2018_ds.hvplot.quadmesh(geo=True, rasterize=True, project=True,\n",
    "                                   xlabel='lon', ylabel='lat', cmap='viridis',\n",
    "                                   tiles='EsriImagery', title='Mean Snow Depth - WY2018') + \\\n",
    "    max_snd_CO_wy2018_ds.hvplot.quadmesh(geo=True, rasterize=True, project=True,\n",
    "                                   xlabel='lon', ylabel='lat', cmap='viridis',\n",
    "                                   tiles='EsriImagery', title='Max Snow Depth - WY2018')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-awareness",
   "metadata": {},
   "source": [
    "### Area Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-jason",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take area-averaged mean at each timestep\n",
    "mean_snd_CO_wy2018_ds = snd_CO_wy2018_ds.mean(['lat', 'lon'])\n",
    "\n",
    "# inspect the dataset\n",
    "mean_snd_CO_wy2018_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-potter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot timeseries (hvplot knows how to plot based on dataset's dimensionality!)\n",
    "mean_snd_CO_wy2018_ds.hvplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-berlin",
   "metadata": {},
   "source": [
    "## Comparing LIS Output\n",
    "\n",
    "Now that we're familiar with the LIS output, let's compare it to other datasets.\n",
    "\n",
    "First, we're going to define a helper function to get the coordinates of the grid cell nearest to a given lat/lon pair:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specific-prime",
   "metadata": {},
   "source": [
    "### LIS (raster) vs. SNODAS (raster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-detail",
   "metadata": {},
   "source": [
    "Draft of LIS vs SNODAS comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-bumper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load SNODAS dataset\n",
    "\n",
    "#snodas depth\n",
    "key = \"SNOWEX-HACKWEEK/SNODAS/snodas_snowdepth_20161001_20200930.zarr\"    \n",
    "snodas_depth_ds = xr.open_zarr(s3.get_mapper(f\"{bucket_name}/{key}\"), consolidated=True)\n",
    "\n",
    "# apply scale factor (0.001 per SNODAS user guide)\n",
    "snodas_depth_ds = snodas_depth_ds * 0.001\n",
    "\n",
    "#snodas swe\n",
    "# key = \"SNOWEX-HACKWEEK/SNODAS/snodas_swe_20161001_20200930.zarr\"\n",
    "# snodas_swe_ds = xr.open_zarr(s3.get_mapper(f\"{bucket_name}/{key}\"), consolidated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-offering",
   "metadata": {},
   "source": [
    "Define helper function to get the (lon, lat) of the nearest grid cell to the given `pt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-french",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_grid(ds, pt):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns the nearest lon and lat to pt in a given Dataset (ds).\n",
    "    \n",
    "    pt : input point, tuple (longitude, latitude)\n",
    "    output:\n",
    "        lon, lat\n",
    "    \"\"\"\n",
    "    \n",
    "    if all(coord in list(ds.coords) for coord in ['lat', 'lon']):\n",
    "        df_loc = ds[['lon', 'lat']].to_dataframe().reset_index()\n",
    "    else:\n",
    "        df_loc = ds[['orig_lon', 'orig_lat']].isel(time=0).to_dataframe().reset_index()\n",
    "    \n",
    "    loc_valid = df_loc.dropna()\n",
    "    pts = loc_valid[['lon', 'lat']].to_numpy()\n",
    "    idx = distance.cdist([pt], pts).argmin()\n",
    "    \n",
    "    return loc_valid['lon'].iloc[idx], loc_valid['lat'].iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-presence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNODAS is at finer resolution than LIS, reinterpolate to LIS output? Has performance penalty...\n",
    "# snodas_depth_ds.interp_like(lis_output_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-taylor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get lon, lat of snodas grid cell nearest to the LIS coordinates we used earlier\n",
    "snodas_ts_lon, snodas_ts_lat = nearest_grid(snodas_depth_ds, (ts_lon, ts_lat))\n",
    "\n",
    "# define a date range to plot (shorter = quicker for demo)\n",
    "start_date, end_date = ('2018-01-01', '2018-03-01')\n",
    "plot_daterange = slice(start_date, end_date)\n",
    "\n",
    "# plot LIS vs SNODAS snow depth\n",
    "(snodas_depth_ds.sel(lon=snodas_ts_lon, lat=snodas_ts_lat, time=plot_daterange).hvplot(ylabel='Snow Depth (m)', label='SNODAS') * \\\n",
    "    lis_output_ds['SnowDepth_tavg'].sel(lat=ts_lat, lon=ts_lon, time=plot_daterange).hvplot(label='LIS')).opts(title=f'Snow Depth @ Lon: {ts_lon}, Lat: {ts_lat}', legend_position='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-assistant",
   "metadata": {},
   "source": [
    "### LIS (raster) vs. SNOTEL (point)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continental-mexico",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-crest",
   "metadata": {},
   "source": [
    "See the following example for how to build an interactive widget for creating comparison time series of LIS, SNODAS, and SNOTEL, all together in one plot."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
