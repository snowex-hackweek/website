{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "coordinated-cycle",
   "metadata": {},
   "source": [
    "# Downloading datasets for the thermal IR tutorial\n",
    "\n",
    "Download through NASA EarthData API https://nsidc.org/data/SNEX20_VPTS_Raw/versions/1\n",
    "\n",
    "**At the conclusion of this tutorial, you will be able to:**\n",
    "- Download datasets through the NASA Earthdata API\n",
    "- Use data conversion tools to create geotiff files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "threaded-arrest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages we'll need\n",
    "import pandas as pd\n",
    "from earthdata_api import earthdata_granule_search # for interfacing with the NASA Earthdata API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "racial-drunk",
   "metadata": {},
   "source": [
    "I'm going to use this `earthdata_granule_search` function which can return a list of URLs that we can then use a utility like wget (with our EarthData credentials) to download the files. Alternatively, we could search on the Earthdata website itself and download through the web interface (which is quite a good tool in my opinion)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-solomon",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "First, set up some of the search parameters I'll use for all the datasets I want to find: a bounding box for the Grand Mesa area, and a date/time that I want to look for. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-robin",
   "metadata": {},
   "source": [
    "Define a bounding box at the study area I want to look at. Earthdata will return anything that overlaps this spot.\n",
    "What I'm specifying is a a tiny area, basicallcy a point search, but we can expand to a larger area if needed.\n",
    "\n",
    "Format like `bounding_box = [ lower left longitude, lower left latitude, upper right longitude, upper right latitude ]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "wicked-polish",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounding_box = [-108.19, 39.03, -108.18, 39.04] # a little box on top of Grand Mesa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-supervisor",
   "metadata": {},
   "source": [
    "I am going to look at one specifc day, but we could search multiple dates/times that don't have to be a continous time period by providing a list of date/times (that was my original motivation in writing this function). Use either pandas.Timestamp, datetime.datetime, or numpy.datetime64 objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "involved-arena",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestampsUTC = pd.Timestamp('2020-2-08') # February 8, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-davis",
   "metadata": {},
   "source": [
    "---\n",
    "Now we can download the first dataset we want, ASTER satellite images. Specificallfy we are going to download the AST_L1T product. Looking its product documentation we see that it is available either as an HDF file with all bands, or as tif files with select visible or thermal bands.\n",
    "\n",
    "We want to download the hdf file, which we'll later convert to separate geotiffs for each band."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ready-phone",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information for the ASTER product I want\n",
    "product=\"AST_L1T\" # product name\n",
    "version=\"003\" # product version\n",
    "ext=\"hdf\" # geotiff files only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-billion",
   "metadata": {},
   "source": [
    "To make sure we find the images from this day, we need to specify a window in time to search, which starts at our \"timestampsUTC\" time.\n",
    "\n",
    "To define this time window we specify two things, the size of the time window from our start time, and units of that size value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "reasonable-floating",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_time_window=1 # specify that we want to have a time window of 1\n",
    "search_time_window_units=\"d\" # specify time window units of Days, which together gives us 1 day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-easter",
   "metadata": {},
   "source": [
    "We also want to save the list of data access URLs so we can use wget to download them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "formal-break",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_filepath=\"data/aster_download_list.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sophisticated-wholesale",
   "metadata": {},
   "source": [
    "Finally, make the API call to get the list of files that match our search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cubic-blocking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only including hdf files in list\n",
      "Writing list of URLs to data/aster_download_list.txt\n",
      "['https://e4ftl01.cr.usgs.gov//ASTER_L1T/ASTT/AST_L1T.003/2020.02.08/AST_L1T_00302082020180748_20200209065849_17218.hdf']\n"
     ]
    }
   ],
   "source": [
    "url_list = earthdata_granule_search(product, version, \n",
    "                                    bounding_box, timestampsUTC, \n",
    "                                    search_time_window, search_time_window_units, \n",
    "                                    list_filepath, ext)\n",
    "# we can print the list of files, but these are also saved to a file\n",
    "print(url_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-nothing",
   "metadata": {},
   "source": [
    "---\n",
    "**Now we can open a terminal and use wget to download the files that are listed.**\n",
    "\n",
    "Open a terminal and navigate to the data directory for this tutorial\n",
    "\n",
    "`cd data`\n",
    "\n",
    "Use wget to download the files listed in aster_download_list.txt. This requires that you put your Earthdata username wherer it says YOUR_USERNAME. This will also prompt you for your Earthdata password.\n",
    "\n",
    "`wget --http-user=YOUR_USERNAME --ask-password --keep-session-cookies --auth-no-challenge=on -c -i aster_download_list.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-jamaica",
   "metadata": {},
   "source": [
    "**Convert hdf to geotiffs** with ASTERL1T_hdf2tir.py script\n",
    "\n",
    "\"Output data from the ASTERL1T_hdf2tif.py script  is At-Sensor radiance stored as Digital Numbers (DN) in UINT16 data type.\"\n",
    "\n",
    "\"NASA LP DAAC. (2020). Reformat and Georeference ASTER L1T HDF Files Data Prep Script. Ver. 1. NASA EOSDIS Land Processes Distributed Active Archive Center (LP DAAC), USGS/Earth Resources Observation and Science (EROS) Center, Sioux Falls, South Dakota, USA. Accessed July 27, 2020. https://git.earthdata.nasa.gov/projects/LPDUR/repos/aster-l1t/browse\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "previous-leeds",
   "metadata": {},
   "source": [
    "You should now see the AST_L1T geotiff files in your data directory!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-lexington",
   "metadata": {},
   "source": [
    "---\n",
    "Download the ground-based temperature measruements for one site\n",
    "\n",
    "https://nsidc.org/data/SNEX20_VPTS_Raw/versions/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "prescription-breath",
   "metadata": {},
   "outputs": [],
   "source": [
    "product = \"SNEX20_VPTS_Raw\"\n",
    "version = \"1\"\n",
    "list_filepath=\"data/snow_temperature_download_list.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "macro-turning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing list of URLs to data/snow_temperature_download_list.txt\n",
      "['https://n5eil01u.ecs.nsidc.org/DP6/SNOWEX/SNEX20_VPTS_Raw.001/2020.02.05/SNEX20_VPTS_Raw.zip']\n"
     ]
    }
   ],
   "source": [
    "url_list = earthdata_granule_search(product, version, \n",
    "                                    bounding_box, timestampsUTC, \n",
    "                                    search_time_window, search_time_window_units, \n",
    "                                    list_filepath)\n",
    "print(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-decrease",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fancy-perry",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Use wget to download the ground-based snow temperatue data\n",
    "\n",
    "`wget --http-user=YOUR_USERNAME --ask-password --keep-session-cookies --auth-no-challenge=on -c -i snow_temperature_download_list.txt.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-buffalo",
   "metadata": {},
   "source": [
    "Then unzip the folder we just downloaded (`-q` for \"quiet mode\" so it doesn't list every file as it unzips them, `-d` for the directory we want to unzip the files into\n",
    "\n",
    "`unzip -q -d data/SNEX20_VPTS_Raw SNEX20_VPTS_Raw.zip`\n",
    "\n",
    "And you should see the folder \"Level-0\" with the raw ground-based snow temperature data inside now in your data directory\n",
    "\n",
    "The file we want is `data/SNEX20_VPTS_Raw/Level-0/snow-temperature-timeseries/CR10X_GM1_final_storage_1.dat`, this is a comma-delimited text file from the datalogger at snow pit 2S10\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-phrase",
   "metadata": {},
   "source": [
    "Note: The airborne TIR imagery from SnowEx 2020 is not yet publicly available through NSIDC, we will work with a sample image provided in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-saint",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
