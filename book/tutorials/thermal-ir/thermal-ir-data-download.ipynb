{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "infectious-lemon",
   "metadata": {},
   "source": [
    "# Downloading datasets for the thermal IR tutorial\n",
    "\n",
    "This is an optional notebook to demonstrate a method for accessing SnowEx data through the NASA EarthData API. \n",
    "\n",
    "Alternatively, the [thermal-ir-tutorial.ipynb](thermal-ir-tutorial.ipynb) includes a command to download sample data for the tutorial.\n",
    "\n",
    ":::{admonition} Learning Objectives\n",
    "\n",
    "**At the conclusion of this tutorial, you will be able to:**\n",
    "- download datasets through the NASA Earthdata API\n",
    "- use data conversion tools to create geotiff files\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-vietnam",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages we'll need\n",
    "import pandas as pd\n",
    "from earthdata_api import earthdata_granule_search # for interfacing with the NASA Earthdata API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turkish-better",
   "metadata": {},
   "source": [
    "We are going to use this [earthdata_api.earthdata_granule_search](earthdata_api.py) function which can return a list of URLs that we can then use a utility like wget (with our EarthData credentials) to download the files. Alternatively, we could search on the [EarthData](https://earthdata.nasa.gov/) or [NSIDC](nsidc.org/) website itself and download through the web interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-coupon",
   "metadata": {},
   "source": [
    "**First, set up some of the search parameters we'll use for all the datasets we want to find:**\n",
    "* a bounding box for the Grand Mesa area, \n",
    "* and a date/time that we want to look for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-peninsula",
   "metadata": {},
   "source": [
    "When defining a bounding box for the study area we want to look at, Earthdata will return anything that overlaps this spot.\n",
    "\n",
    "What we are specifying  below is a a tiny area, basicallcy a point search, but we can expand to a larger area if needed.\n",
    "\n",
    "Format the bounding box as a list of coordinates like `bounding_box = [ lower left longitude, lower left latitude, upper right longitude, upper right latitude ]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-client",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounding_box = [-108.19, 39.03, -108.18, 39.04] # a little box on top of Grand Mesa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-penetration",
   "metadata": {},
   "source": [
    "We are going to look at one specifc day, but we could search multiple dates/times. Those dates/times don't need to be a continous time period, we could instead provide a list of dates/times. (That was my original motivation to write this function). Use either [pandas.Timestamp](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Timestamp.html), [datetime.datetime](https://docs.python.org/3/library/datetime.html#datetime.datetime), or [numpy.datetime64](https://numpy.org/doc/stable/reference/arrays.datetime.html) objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-configuration",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestampsUTC = pd.Timestamp('2020-2-08') # February 8, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-headquarters",
   "metadata": {},
   "source": [
    "---\n",
    "**Now we can find the first dataset we want, [ASTER](https://asterweb.jpl.nasa.gov/) satellite images.** Specificallfy we are going to download the AST_L1T product. Looking its [product documentation](https://lpdaac.usgs.gov/products/ast_l1tv003/) we see that it is available either as an HDF file with all bands, or as tif files with select visible or thermal bands.\n",
    "\n",
    "We want to download the hdf file, which we'll later convert to separate geotiffs for each band."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-snake",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information for the ASTER product we want\n",
    "product=\"AST_L1T\" # product name\n",
    "version=\"003\" # product version\n",
    "ext=\"hdf\" # geotiff files only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extra-shame",
   "metadata": {},
   "source": [
    "To make sure we find the images from this day, we need to specify a window in time to search, which starts at our `timestampsUTC` time we already defined.\n",
    "\n",
    "To define this time window we specify two things, the size of the time window from our start time, and units of that size value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consecutive-sarah",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_time_window=1 # specify that we want to have a time window of 1\n",
    "search_time_window_units=\"d\" # specify time window units of Days, which together gives us 1 day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-eating",
   "metadata": {},
   "source": [
    "We also want to save the list of data access URLs so we can use wget to download them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-population",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_filepath=\"/tmp/thermal-ir/aster_download_list.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-hospital",
   "metadata": {},
   "source": [
    "**Finally, make the API call to get the list of files that match our search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-intellectual",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = earthdata_granule_search(product, version, \n",
    "                                    bounding_box, timestampsUTC, \n",
    "                                    search_time_window, search_time_window_units, \n",
    "                                    list_filepath, ext)\n",
    "# we can print the list of files, but these are also saved to a file\n",
    "print(url_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-reynolds",
   "metadata": {},
   "source": [
    "**Now we can open a terminal and use wget to download the files that are listed.**\n",
    "\n",
    "Open a terminal and navigate to the data directory for this tutorial\n",
    "\n",
    "`cd /tmp/thermal-ir/`\n",
    "\n",
    "Use wget to download the files listed in aster_download_list.txt. This requires that you put your Earthdata username wherer it says YOUR_USERNAME. This will also prompt you for your Earthdata password.\n",
    "\n",
    "`wget --http-user=YOUR_USERNAME --ask-password --keep-session-cookies --auth-no-challenge=on -c -i aster_download_list.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-florida",
   "metadata": {},
   "source": [
    "**Convert hdf to geotiffs with ASTERL1T_hdf2tir.py script.** When we run this utility, the output data will be at-sensor radiance stored as Digital Numbers (DN) as uint16 data types.\n",
    "\n",
    "The ASTERL1T_hdf2tir.py is from:\n",
    "*NASA LP DAAC. (2020). Reformat and Georeference ASTER L1T HDF Files Data Prep Script. Ver. 1. NASA EOSDIS Land Processes Distributed Active Archive Center (LP DAAC), USGS/Earth Resources Observation and Science (EROS) Center, Sioux Falls, South Dakota, USA. Accessed July 27, 2020. [link](https://git.earthdata.nasa.gov/projects/LPDUR/repos/aster-l1t/browse)*\n",
    "\n",
    "In the terminal, navigate to the directory containing the ASTERL1T_hdf2tir.py script\n",
    "\n",
    "`cd ~/website/book/tutorials/thermal-ir/ast-l1t/`\n",
    "\n",
    "Then run the script as follows to generate geotiff files for each ASTER band\n",
    "\n",
    "`python ASTERL1T_hdf2tif.py /tmp/thermal-ir/`\n",
    "\n",
    "You should now see the AST_L1T geotiff files in the data directory!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-electron",
   "metadata": {},
   "source": [
    "---\n",
    "**Download the second dataset we want, [ground-based temperature measurments](https://nsidc.org/data/SNEX20_VPTS_Raw/versions/1)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-enhancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "product = \"SNEX20_VPTS_Raw\"\n",
    "version = \"1\"\n",
    "list_filepath=\"/tmp/thermal-ir/snow_temperature_download_list.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-helmet",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = earthdata_granule_search(product, version, \n",
    "                                    bounding_box, timestampsUTC, \n",
    "                                    search_time_window, search_time_window_units, \n",
    "                                    list_filepath)\n",
    "print(url_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cultural-condition",
   "metadata": {},
   "source": [
    "**Use wget to download these data**\n",
    "\n",
    "`wget --http-user=YOUR_USERNAME --ask-password --keep-session-cookies --auth-no-challenge=on -c -i snow_temperature_download_list.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-thriller",
   "metadata": {},
   "source": [
    "Then unzip the folder we just downloaded (`-q` for \"quiet mode\" so it doesn't list every file as it unzips them, `-d` for the directory we want to unzip the files into)\n",
    "\n",
    "`unzip -q -d /tmp/thermal-ir/SNEX20_VPTS_Raw SNEX20_VPTS_Raw.zip`\n",
    "\n",
    "And you should see the folder \"Level-0\" with the raw ground-based snow temperature data inside now in your data directory\n",
    "\n",
    "The file we want is `/tmp/thermal-ir/SNEX20_VPTS_Raw/Level-0/snow-temperature-timeseries/CR10X_GM1_final_storage_1.dat`, this is a comma-delimited text file from the datalogger at snow pit 2S10\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-street",
   "metadata": {},
   "source": [
    ":::{admonition} Note:\n",
    ":class: warning\n",
    "The airborne TIR imagery from SnowEx 2020 is not yet publicly available through NSIDC, we will work with a sample image provided in this tutorial.\n",
    ":::\n",
    "\n",
    "\n",
    "\n",
    "You can access all data needed for the tutorial by running the following command in a terminal:\n",
    "\n",
    "`aws s3 sync --quiet s3://snowex-data/tutorial-data/thermal-ir/ /tmp/thermal-ir/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-wiring",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
