{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "reliable-specialist",
   "metadata": {},
   "source": [
    "# Thermal IR Tutorial 0: Downloading datasets for this tutorial\n",
    "\n",
    "Download through NASA EarthData API https://nsidc.org/data/SNEX20_VPTS_Raw/versions/1\n",
    "\n",
    "**At the conclusion of this tutorial, you will be able to:**\n",
    "- Download datasets through the NASA Earthdata API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "boxed-shanghai",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages we'll need\n",
    "import pandas as pd\n",
    "from earthdata_api import earthdata_granule_search # for interfacing with the NASA Earthdata API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-longitude",
   "metadata": {},
   "source": [
    "I'm going to use this `earthdata_granule_search` function which can return a list of URLs that we can then use a utility like wget (with our EarthData credentials) to download the files. Alternatively, we could search on the Earthdata website itself and download through the web interface (which is quite a good tool in my opinion)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-possession",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "First, set up some of the search parameters I'll use for all the datasets I want to find: a bounding box for the Grand Mesa area, and a date/time that I want to look for. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-anthony",
   "metadata": {},
   "source": [
    "Define a bounding box at the study area I want to look at. Earthdata will return anything that overlaps this spot.\n",
    "What I'm specifying is a a tiny area, basicallcy a point search, but we can expand to a larger area if needed.\n",
    "\n",
    "Format like `bounding_box = [ lower left longitude, lower left latitude, upper right longitude, upper right latitude ]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "frequent-cliff",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounding_box = [-108.19, 39.03, -108.18, 39.04] # a little box on top of Grand Mesa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-anxiety",
   "metadata": {},
   "source": [
    "I am going to look at one specifc day, but we could search multiple dates/times that don't have to be a continous time period by providing a list of date/times (that was my original motivation in writing this function). Use either pandas.Timestamp, datetime.datetime, or numpy.datetime64 objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "weighted-thesaurus",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestampsUTC = pd.Timestamp('2020-2-08') # February 8, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-drilling",
   "metadata": {},
   "source": [
    "---\n",
    "Now we can download the first dataset we want, ASTER satellite images. Specificallfy we are going to download the AST_L1T product. Looking its product documentation we see that it is available either as an HDF file with all bands, or as two tif files, one with visible bands and the other with the thermal bands. \n",
    "\n",
    "We'll use the geotiff files today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "addressed-thirty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product information for the ASTER products I want\n",
    "product=\"AST_L1T\" # product name\n",
    "version=\"003\" # product version\n",
    "ext=\"tif\" # geotiff files only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-pioneer",
   "metadata": {},
   "source": [
    "To make sure we find the images from this day, we need to specify a window in time to search, which starts at our \"timestampsUTC\" time.\n",
    "\n",
    "To define this time window we specify two things, the size of the time window from our start time, and units of that size value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "healthy-auditor",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_time_window=1 # specify that we want to have a time window of 1\n",
    "search_time_window_units=\"d\" # specify time window units of Days, which together gives us 1 day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-piano",
   "metadata": {},
   "source": [
    "We also want to save the list of data access URLs so we can use wget to download them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ready-wrestling",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_filepath=\"data/aster_download_list.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-filling",
   "metadata": {},
   "source": [
    "Finally, make the API call to get the list of files that match our search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "greek-swimming",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only including tif files in list\n",
      "Writing list of URLs to data/aster_download_list.txt\n",
      "['https://e4ftl01.cr.usgs.gov//ASTER_L1T/ASTT/AST_L1T.003/2020.02.08/AST_L1T_00302082020180748_20200209065849_17218_T.tif', 'https://e4ftl01.cr.usgs.gov//ASTER_L1T/ASTT/AST_L1T.003/2020.02.08/AST_L1T_00302082020180748_20200209065849_17218_V.tif']\n"
     ]
    }
   ],
   "source": [
    "url_list = earthdata_granule_search(product, version, \n",
    "                                    bounding_box, timestampsUTC, \n",
    "                                    search_time_window, search_time_window_units, \n",
    "                                    list_filepath, ext)\n",
    "# we can print the list of files, but these are also saved to a file\n",
    "print(url_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-trainer",
   "metadata": {},
   "source": [
    "Now we can open a terminal and use wget to download the files that are listed.\n",
    "\n",
    "Open a terminal and navigate to the data directory for this tutorial\n",
    "\n",
    "`cd data`\n",
    "\n",
    "Use wget to download the files listed in aster_download_list.txt. This requires that you put your Earthdata username wherer it says YOUR_USERNAME. This will also prompt you for your Earthdata password.\n",
    "\n",
    "`wget --http-user=YOUR_USERNAME --ask-password --keep-session-cookies --auth-no-challenge=on -c -i aster_download_list.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-soccer",
   "metadata": {},
   "source": [
    "You should now see the AST_L1T geotiff files in your data directory!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-fellowship",
   "metadata": {},
   "source": [
    "---\n",
    "Now download the airborne TIR imagery for the same day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "taken-verse",
   "metadata": {},
   "outputs": [],
   "source": [
    "#product = \"\"\n",
    "#version = \"\"\n",
    "#list_filepath=\"data/airborne_ir_download_list.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "raised-vocabulary",
   "metadata": {},
   "outputs": [],
   "source": [
    "#url_list = earthdata_granule_search(product, version, \n",
    "#                                    bounding_box, timestampsUTC, \n",
    "#                                    search_time_window, search_time_window_units, \n",
    "#                                    list_filepath)\n",
    "#print(url_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-orientation",
   "metadata": {},
   "source": [
    "---\n",
    "Finally download the ground-based temperature measruements for one site\n",
    "\n",
    "https://nsidc.org/data/SNEX20_VPTS_Raw/versions/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "animated-scholar",
   "metadata": {},
   "outputs": [],
   "source": [
    "product = \"SNEX20_VPTS_Raw\"\n",
    "version = \"1\"\n",
    "list_filepath=\"data/snow_temperature_download_list.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "enabling-southeast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing list of URLs to data/snow_temperature_download_list.txt\n",
      "['https://n5eil01u.ecs.nsidc.org/DP6/SNOWEX/SNEX20_VPTS_Raw.001/2020.02.05/SNEX20_VPTS_Raw.zip']\n"
     ]
    }
   ],
   "source": [
    "url_list = earthdata_granule_search(product, version, \n",
    "                                    bounding_box, timestampsUTC, \n",
    "                                    search_time_window, search_time_window_units, \n",
    "                                    list_filepath)\n",
    "print(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-combine",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "rubber-columbus",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Use wget to download the airborne IR imagery and the ground-based snow temperatue data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-arrow",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
