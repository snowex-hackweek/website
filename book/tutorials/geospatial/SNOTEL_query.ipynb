{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "subsequent-engineer",
   "metadata": {},
   "source": [
    "# Dynamic Query of SNOTEL data\n",
    "SnowEx Hackweek  \n",
    "July 13, 2021\n",
    "\n",
    "David Shean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiet-marathon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cutting-toilet",
   "metadata": {},
   "source": [
    "## Read a bit about SNOTEL data for the Western U.S.\n",
    "\n",
    "https://www.wcc.nrcs.usda.gov/snow/\n",
    "\n",
    "This is actually a nice web interface, with some advanced querying and interactive visualization.  You can also download formatted ASCII files (csv) for analysis.  This is great for one-time projects, but it's nice to have reproducible code that can be updated as new data appear, without manual steps.  That's what we're going to do here.\n",
    "\n",
    "### About SNOTEL sites and data:\n",
    "* https://www.wcc.nrcs.usda.gov/about/mon_automate.html\n",
    "* https://www.wcc.nrcs.usda.gov/snotel/snotel_sensors.html\n",
    "* https://directives.sc.egov.usda.gov/OpenNonWebContent.aspx?content=27630.wba\n",
    "\n",
    "### Sample plots for SNOTEL site at Paradise, WA (south side of Mt. Rainier)\n",
    "* https://www.nwrfc.noaa.gov/snow/snowplot.cgi?AFSW1\n",
    "* We will reproduce some of these plots/metrics in this tutorial\n",
    "\n",
    "### Interactive dashboard\n",
    "* https://climate.washington.edu/climate-data/snowdepth/\n",
    "\n",
    "### Snow today\n",
    "* https://nsidc.org/reports/snow-today"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-aruba",
   "metadata": {},
   "source": [
    "# CUAHSI WOF server and automated Python data queries\n",
    "\n",
    "We are going to use a server set up by CUAHSI to serve the SNOTEL data, using a standardized database storage format and query structure.  You don't need to worry about this, but can quickly review the following:\n",
    "* http://hiscentral.cuahsi.org/pub_network.aspx?n=241 \n",
    "* http://his.cuahsi.org/wofws.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-crisis",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the latest CUAHSI API endpoint\n",
    "#http://his.cuahsi.org/wofws.html\n",
    "wsdlurl = 'http://hydroportal.cuahsi.org/Snotel/cuahsi_1_1.asmx?WSDL'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uniform-cleaners",
   "metadata": {},
   "source": [
    "### Acronym soup\n",
    "* SNOTEL = Snow Telemetry\n",
    "* CUAHSI = Consortium of Universities for the Advancement of Hydrologic Science, Inc\n",
    "* WOF = WaterOneFlow\n",
    "* WSDL = Web Services Description Language\n",
    "* USDA = United States Department of Agriculture\n",
    "* NRCS = National Resources Conservation Service\n",
    "* AWDB = Air-Water Database\n",
    "\n",
    "### Python options\n",
    "\n",
    "There are a few packages out there that offer convenience functions to query the online SNOTEL databases and unpack the results.  \n",
    "* climata (https://pypi.org/project/climata/) - last commit Sept 2017 (not a good sign)\n",
    "* ulmo (https://github.com/ulmo-dev/ulmo) - last commit Oct 2020 (will be superseded by a package called Quest, but still maintained by [Emilio Mayorga](https://apl.uw.edu/people/profile.php?last_name=Mayorga&first_name=Emilio) over at UW APL)\n",
    "\n",
    "You can also write your own queries using the Python `requests` module and some built-in XML parsing libraries.\n",
    "\n",
    "Hopefully not overwhelming amount of information - let's just go with ulmo for now.  I've done most of the work to prepare functions for querying and processing the data.  Once you wrap your head around all of the acronyms, it's pretty simple, basically running a few functions here: https://ulmo.readthedocs.io/en/latest/api.html#module-ulmo.cuahsi.wof\n",
    "\n",
    "We will use ulmo with daily data for this exercise, but please feel free to experiment with hourly data, other variables or other approaches to fetch SNOTEL data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-chambers",
   "metadata": {},
   "source": [
    "### Important ulmo installation note\n",
    "\n",
    "We're going to use the latest development version of ulmo, straight from the github source!  This is a good exercise, and will show you how to install a package directly from source code on github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-devices",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install directly from github repo main branch\n",
    "%pip install -q git+https://github.com/ulmo-dev/ulmo.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-capability",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Despite warning, shouldn't need to restart kernel if all goes well\n",
    "import ulmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-watch",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import contextily as ctx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wicked-titanium",
   "metadata": {},
   "source": [
    "# Part 1: Spatial Query SNOTEL sites\n",
    "* Use the ulmo cuahsi interface and the `get_sites` function to fetch available site metadata from server\n",
    "* This will return a Python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subtle-reward",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = ulmo.cuahsi.wof.get_sites(wsdlurl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-boutique",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preview first item in dictionary\n",
    "next(iter(sites.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-dividend",
   "metadata": {},
   "source": [
    "## Store the dictionary as a Pandas DataFrame called `sites_df`\n",
    "* See the Pandas `from_dict` function\n",
    "* Use `orient` option so the sites comprise the DataFrame index, with columns for 'name', 'elevation_m', etc\n",
    "* Use the `dropna` method to remove any empty records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-specific",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_df = pd.DataFrame.from_dict(sites, orient='index').dropna()\n",
    "sites_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-catalog",
   "metadata": {},
   "source": [
    "## Clean up the DataFrame and prepare Point geometry objects\n",
    "* Convert `'location'` column (contains dictionary with `'latitude'` and `'longitude'` values) to Shapely `Point` objects\n",
    "* Store as a new `'geometry'` column (needed by GeoPandas)\n",
    "* Drop the `'location'` column, as this is no longer needed\n",
    "* Update the `dtype` of the `'elevation_m'` column to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-finance",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_df['geometry'] = [Point(float(loc['longitude']), float(loc['latitude'])) for loc in sites_df['location']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-sunglasses",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_df = sites_df.drop(columns='location')\n",
    "sites_df = sites_df.astype({\"elevation_m\":float})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-chicago",
   "metadata": {},
   "source": [
    "## Review output\n",
    "* Take a moment to familiarize yourself with the DataFrame structure and different columns.\n",
    "* Note that the index is a set of strings with format 'SNOTEL:1000_OR_SNTL'\n",
    "* Extract the first record with `loc`\n",
    "    * Review the `'site_property'` dictionary - could parse this and store as separate fields in the DataFrame if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-fancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-insight",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_df.loc['SNOTEL:301_CA_SNTL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-tradition",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_df.loc['SNOTEL:301_CA_SNTL']['site_property']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-processing",
   "metadata": {},
   "source": [
    "## Convert to a Geopandas GeoDataFrame\n",
    "* We already have `'geometry'` column, but still need to define the `crs` of the point coordiantes\n",
    "* Note the number of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-working",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_gdf_all = gpd.GeoDataFrame(sites_df, crs='EPSG:4326')\n",
    "sites_gdf_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-ecology",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_gdf_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-zimbabwe",
   "metadata": {},
   "source": [
    "## Create a scatterplot showing elevation values for all sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-thirty",
   "metadata": {},
   "outputs": [],
   "source": [
    "#geojson of state polygons\n",
    "states_url = 'http://eric.clst.org/assets/wiki/uploads/Stuff/gz_2010_us_040_00_5m.json'\n",
    "states_gdf = gpd.read_file(states_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-sheep",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10,6))\n",
    "sites_gdf_all.plot(ax=ax, column='elevation_m', markersize=3, cmap='inferno', legend=True)\n",
    "#This prevents matplotlib from updating the axes extent (states polygons cover larger area than SNOTEL points)\n",
    "ax.autoscale(False)\n",
    "states_gdf.plot(ax=ax, facecolor='none', edgecolor='k', alpha=0.3);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-kenya",
   "metadata": {},
   "source": [
    "## Exclude the Alaska (AK) points to isolate points over Western U.S.\n",
    "* Simple appraoch is to remove points where the site name contains 'AK' with attribute filter\n",
    "* Note the number of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-invite",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_gdf_conus = sites_gdf_all[~(sites_gdf_all.index.str.contains('AK'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-bacon",
   "metadata": {},
   "source": [
    "* Alternatively, can use a spatial filter (see GeoPandas `cx` indexer functionality for a bounding box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-dollar",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xmin, xmax, ymin, ymax = [-126, 102, 30, 50]\n",
    "#sites_gdf_conus = sites_gdf_all.cx[xmin:xmax, ymin:ymax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-suicide",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_gdf_conus.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-motorcycle",
   "metadata": {},
   "source": [
    "## Update your scatterplot as sanity check\n",
    "* Should look something like the Western U.S."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "balanced-question",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10,6))\n",
    "sites_gdf_conus.plot(ax=ax, column='elevation_m', markersize=3, cmap='inferno', legend=True)\n",
    "ax.autoscale(False)\n",
    "states_gdf.plot(ax=ax, facecolor='none', edgecolor='k', alpha=0.3);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-arena",
   "metadata": {},
   "source": [
    "## Export SNOTEL site GeoDataFrame as a geojson\n",
    "* Maybe useful for other purposes, and can avoid all of the above processing, just load directly with geopandas `read_file`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-movie",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_fn = 'snotel_conus_sites.json'\n",
    "if not os.path.exists(sites_fn):\n",
    "    sites_gdf_conus.to_file(sites_fn, driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-recall",
   "metadata": {},
   "source": [
    "# Part 2: Spatial filter points by polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-playlist",
   "metadata": {},
   "source": [
    "## Load Grand Mesa Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-syndication",
   "metadata": {},
   "outputs": [],
   "source": [
    "gm_poly_fn = 'grand_mesa_poly.geojson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-nickel",
   "metadata": {},
   "outputs": [],
   "source": [
    "gm_poly = gpd.read_file(gm_poly_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-envelope",
   "metadata": {},
   "outputs": [],
   "source": [
    "gm_poly.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-utilization",
   "metadata": {},
   "source": [
    "## Isolate Polygon geometry within GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-edwards",
   "metadata": {},
   "outputs": [],
   "source": [
    "gm_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-taxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "gm_poly.total_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-pennsylvania",
   "metadata": {},
   "outputs": [],
   "source": [
    "gm_poly.iloc[0] #Still a GeoSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stylish-sight",
   "metadata": {},
   "outputs": [],
   "source": [
    "gm_geom = gm_poly.iloc[0].geometry\n",
    "gm_geom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-company",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gm_geom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-union",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(gm_geom.exterior.coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-citizenship",
   "metadata": {},
   "outputs": [],
   "source": [
    "gm_geom.bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-amber",
   "metadata": {},
   "source": [
    "## Generate boolean index for points that intersect the polygon\n",
    "* This will return a new GeoDataSeries with True/False values for each record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-block",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = sites_gdf_all.intersects(gm_geom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-childhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-nowhere",
   "metadata": {},
   "source": [
    "## Use fancy indexing to isolate points and return new GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-adams",
   "metadata": {},
   "outputs": [],
   "source": [
    "gm_snotel_sites = sites_gdf_all.loc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-living",
   "metadata": {},
   "outputs": [],
   "source": [
    "gm_snotel_sites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laughing-jacket",
   "metadata": {},
   "source": [
    "## Quick plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-reflection",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10,6))\n",
    "gm_snotel_sites.plot(ax=ax, column='elevation_m', markersize=20, edgecolor='k', cmap='inferno', \\\n",
    "                  legend=True, legend_kwds={'label':'Elevation (m)'})\n",
    "#ctx.add_basemap(ax=ax, crs=gm_snotel_sites.crs, source=ctx.providers.Stamen.Terrain)\n",
    "ax.set_title('Grand Mesa SNOTEL Stations');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "touched-offset",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.pandas\n",
    "from geoviews import tile_sources as gvts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-wrestling",
   "metadata": {},
   "outputs": [],
   "source": [
    "gm_snotel_sites.hvplot(hover_cols=['index','name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-highland",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_tiles = gvts.EsriImagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-theorem",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_tiles * gm_snotel_sites.to_crs('EPSG:3857').hvplot(hover_cols=['index','name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-zimbabwe",
   "metadata": {},
   "source": [
    "# Part 2: Time series analysis for one station"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-horse",
   "metadata": {},
   "source": [
    "https://wcc.sc.egov.usda.gov/nwcc/site?sitenum=622&state=co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-arrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "sitecode = gm_snotel_sites.index[-1]\n",
    "sitecode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modern-browse",
   "metadata": {},
   "source": [
    "### Get available measurements for this site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-adjustment",
   "metadata": {},
   "outputs": [],
   "source": [
    "ulmo.cuahsi.wof.get_site_info(wsdlurl, sitecode)['series'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-sacrifice",
   "metadata": {},
   "source": [
    "* _H = \"hourly\"\n",
    "* _D = \"daily\"\n",
    "* _sm, _m = \"monthly\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-hypothesis",
   "metadata": {},
   "source": [
    "## Let's consider the 'SNOTEL:SNWD_D' variable (Daily Snow Depth)\n",
    "* Assign 'SNOTEL:SNWD_D' to a variable named `variablecode`\n",
    "* Get some information about the variable using `get_variable_info` method\n",
    "    * Note the units, nodata value, etc.\n",
    "* Note: The snow depth records are usually shorter/noisier than the SWE records for SNOTEL sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-methodology",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Daily SWE\n",
    "#variablecode = 'SNOTEL:WTEQ_D'\n",
    "#Daily snow depth\n",
    "variablecode = 'SNOTEL:SNWD_D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-enforcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hourly SWE\n",
    "#variablecode = 'SNOTEL:WTEQ_H'\n",
    "#Hourly snow depth\n",
    "#variablecode = 'SNOTEL:SNWD_H'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-testing",
   "metadata": {},
   "outputs": [],
   "source": [
    "ulmo.cuahsi.wof.get_variable_info(wsdlurl, variablecode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-ribbon",
   "metadata": {},
   "source": [
    "## Define a function to fetch data\n",
    "* I've done this for you, but please review the comments and steps to see what is going on under the hood\n",
    "* You'll probably have to do similar data wrangling for another project at some point in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-sunglasses",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get current datetime\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "def snotel_fetch(sitecode, variablecode='SNOTEL:SNWD_D', start_date='1950-10-01', end_date=today):\n",
    "    #print(sitecode, variablecode, start_date, end_date)\n",
    "    values_df = None\n",
    "    try:\n",
    "        #Request data from the server\n",
    "        site_values = ulmo.cuahsi.wof.get_values(wsdlurl, sitecode, variablecode, start=start_date, end=end_date)\n",
    "        #Convert to a Pandas DataFrame   \n",
    "        values_df = pd.DataFrame.from_dict(site_values['values'])\n",
    "        #Parse the datetime values to Pandas Timestamp objects\n",
    "        values_df['datetime'] = pd.to_datetime(values_df['datetime'], utc=True)\n",
    "        #Set the DataFrame index to the Timestamps\n",
    "        values_df = values_df.set_index('datetime')\n",
    "        #Convert values to float and replace -9999 nodata values with NaN\n",
    "        values_df['value'] = pd.to_numeric(values_df['value']).replace(-9999, np.nan)\n",
    "        #Remove any records flagged with lower quality\n",
    "        values_df = values_df[values_df['quality_control_level_code'] == '1']\n",
    "    except:\n",
    "        print(\"Unable to fetch %s\" % variablecode)\n",
    "\n",
    "    return values_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promotional-protection",
   "metadata": {},
   "source": [
    "## Use this function to get the full 'SNOTEL:SNWD_D' record for one station\n",
    "* Inspect the results\n",
    "* We used a dummy start date of Jan 1, 1950.  What is the actual the first date returned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-suffering",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all records, can filter later\n",
    "start_date = datetime(1950,1,1)\n",
    "end_date = datetime.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-tyler",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sitecode)\n",
    "values_df = snotel_fetch(sitecode, variablecode, start_date, end_date)\n",
    "values_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-muslim",
   "metadata": {},
   "outputs": [],
   "source": [
    "values_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-panama",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get number of decimal years between first and last observation\n",
    "nyears = (values_df.index.max() - values_df.index.min()).days/365.25\n",
    "nyears"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-spice",
   "metadata": {},
   "source": [
    "## Create a quick plot to view the time series\n",
    "* Take a moment to inspect the `value` column, which is where the `SNWD_D` values are stored\n",
    "* Sanity check thought question: *What are the units again?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-stadium",
   "metadata": {},
   "outputs": [],
   "source": [
    "values_df.hvplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boxed-vulnerability",
   "metadata": {},
   "source": [
    "## Compute the integer day of year (doy) and integer day of water year (dowy)\n",
    "* Can get doy for each record with `df.index.dayofyear`\n",
    "    * Can compute on the fly, but add a new column to store these values\n",
    "    * https://pandas.pydata.org/pandas-docs/version/0.19/generated/pandas.DatetimeIndex.dayofyear.html\n",
    "* For the day of water year, you'll need to offset by 9 months, then compute day of year\n",
    "    * https://en.wikipedia.org/wiki/Water_year\n",
    "    * Add another column to store these values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-costa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add DOY and DOWY column\n",
    "#Need to revisit for leap year support\n",
    "def add_dowy(df, col=None):\n",
    "    if col is None:\n",
    "        df['doy'] = df.index.dayofyear\n",
    "    else:\n",
    "        df['doy'] = df[col].dayofyear\n",
    "    #df['dowy'] = (df['doy'].index - pd.DateOffset(months=9)).dayofyear\n",
    "    # Sept 30 is doy 273\n",
    "    df['dowy'] = df['doy'] - 273\n",
    "    df.loc[df['dowy'] <= 0, 'dowy'] += 365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-registrar",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_dowy(values_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-result",
   "metadata": {},
   "source": [
    "## Compute statistics for each day of water year, using values from all years\n",
    "* Seems like a Pandas groupby/agg might work here\n",
    "* Stats should at least include min, max, mean, and median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-northeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_list = ['count','min','max','mean','std','median','mad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-convenience",
   "metadata": {},
   "outputs": [],
   "source": [
    "doy_stats = values_df.groupby('dowy').agg(stat_list)['value']\n",
    "doy_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-music",
   "metadata": {},
   "source": [
    "## Create a plot of these aggregated dowy values\n",
    "* Your output independent variable (x-axis) should be day of water year (1-366), and dependent variable (y-axis) should be median value for that day of year, computed using values from all available years\n",
    "    * You may have to explicitly specify the x and y valuese for your plot function\n",
    "* Something like the 30-year mean and median here: https://www.nwrfc.noaa.gov/snow/plot_SWE.php?id=AFSW1\n",
    "* Extra credit: add shaded regions for standard deviation or normalized median absolute deviation (nmad) for each doy to show spread in values over the full record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-detection",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "for stat in ['min','max','mean','median']:\n",
    "    ax.plot(doy_stats.index, doy_stats[stat], label=stat)\n",
    "\n",
    "ax.fill_between(doy_stats.index, doy_stats['mean'] - doy_stats['std'], doy_stats['mean'] + doy_stats['std'], \\\n",
    "                color='lightgrey', label='1-std')\n",
    "\n",
    "title = f'{sitecode}: \\n{values_df.index.min().date()} to {values_df.index.max().date()} ({nyears:.2f} years)'\n",
    "\n",
    "ax.set_title(title)\n",
    "ax.set_xlabel('Day of Water Year (Oct 1)')\n",
    "ax.set_ylabel('Snow depth (in)')\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "ax.set_xlim(0,366)\n",
    "ax.set_ylim(bottom=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-anatomy",
   "metadata": {},
   "source": [
    "## Add the daily snow depth values for the current water year\n",
    "* Can use pandas indexing here with simple strings ('YYYY-MM-DD'), or Timestamp objects\n",
    "    * Standard slicing also works with `:`\n",
    "* Make sure to `dropna` to remove any records missing data\n",
    "* Add this to your plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-democracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define variable to store current year\n",
    "curr_y = datetime.now().year\n",
    "curr_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-particular",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_wy = values_df.loc['2019-10-1':].dropna()\n",
    "df_wy = values_df.loc[f'{curr_y-1}-10-1':].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "severe-examination",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize=(10,5))\n",
    "for stat in ['min','max','mean','median']:\n",
    "    ax.plot(doy_stats.index, doy_stats[stat], label=stat)\n",
    "\n",
    "ax.fill_between(doy_stats.index, doy_stats['mean'] - doy_stats['std'], doy_stats['mean'] + doy_stats['std'], \\\n",
    "                color='lightgrey', label='1-std')\n",
    "\n",
    "ax.set_title(title)\n",
    "ax.set_xlabel('Day of Water Year')\n",
    "ax.set_ylabel('Snow depth (in)')\n",
    "ax.grid()\n",
    "ax.plot(df_wy['dowy'], df_wy['value'], marker='.', color='k', ls='none', label='Current WY')\n",
    "ax.legend()\n",
    "ax.set_xlim(0,366)\n",
    "ax.set_ylim(bottom=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-instrumentation",
   "metadata": {},
   "source": [
    "## What is the percentage of \"normal\" snow depth (as defined by long-term median for this dowy) at this site for April 1 of current year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-motion",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wy.loc[f'{curr_y}-4-1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-population",
   "metadata": {},
   "outputs": [],
   "source": [
    "apr1_doy = df_wy.loc[f'{curr_y}-4-1']['dowy']\n",
    "apr1_doy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-kitchen",
   "metadata": {},
   "outputs": [],
   "source": [
    "doy_stats.loc[apr1_doy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-explanation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Percent of normal\n",
    "perc_normal = 100 * df_wy.loc[f'{curr_y}-4-1']['value']/doy_stats.loc[apr1_doy]['median']\n",
    "print(f'{perc_normal:0.2f}% percent of normal snow depth on DOWY {apr1_doy} in {curr_y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-budget",
   "metadata": {},
   "source": [
    "## Index by date or date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-winter",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt1 = '2017-2-1'\n",
    "dt2 = '2017-2-7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-window",
   "metadata": {},
   "outputs": [],
   "source": [
    "values_df.loc[dt1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-amazon",
   "metadata": {},
   "outputs": [],
   "source": [
    "values_df.loc[dt1:dt2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-lending",
   "metadata": {},
   "source": [
    "## Query multiple sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-puppy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define an empty dictionary to store returns for each site\n",
    "value_dict = {}\n",
    "for i, sitecode in enumerate(gm_snotel_sites.index):\n",
    "    print('%i of %i sites: %s' % (i+1, len(gm_snotel_sites.index), sitecode))\n",
    "    out = snotel_fetch(sitecode, variablecode)\n",
    "    if out is not None:\n",
    "        value_dict[sitecode] = out['value']\n",
    "#Convert the dictionary to a DataFrame, automatically handles different datetime ranges (nice!)\n",
    "multi_df = pd.DataFrame.from_dict(value_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-baghdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-advertising",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_df.hvplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-driver",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hack to remove bad measurements\n",
    "multi_df[multi_df > 190] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-swimming",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_df.hvplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-priority",
   "metadata": {},
   "source": [
    "## Determine Pearson's correlation coefficient for the two time series\n",
    "* https://en.wikipedia.org/wiki/Pearson_correlation_coefficient\n",
    "* See the Pandas `corr` method\n",
    "    * This should properly handle nan under the hood!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-quest",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-advancement",
   "metadata": {},
   "source": [
    "Highly correlated snow depth records for these two sites!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-hostel",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_df.dropna().hvplot(kind='scatter', x='SNOTEL:622_CO_SNTL', y='SNOTEL:682_CO_SNTL', aspect='equal', s=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
